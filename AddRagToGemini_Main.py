import os
import sys
import asyncio
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Dict, Any

# LlamaIndex RAG Components
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.chat_engine.types import BaseChatEngine
from llama_index.core.chat_engine import CondensePlusContextChatEngine

from google import genai
from google.genai import types

import uvicorn


# --- 1. è¨­å®šã®èª­ã¿è¾¼ã¿ã¨ç’°å¢ƒãƒã‚§ãƒƒã‚¯ ---
class Settings(BaseSettings):
    """pydantic-settings ãŒ .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã¿ã¾ã™ã€‚"""
    model_config = SettingsConfigDict(
        env_file=".env", env_file_encoding="utf-8", extra='ignore'
    )
    GEMINI_API_KEY: str

settings = Settings()



if not settings.GEMINI_API_KEY:
    print("FATAL: 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'.env' ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)
    
    
# --- 2. RAG ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ– ---
def initialize_rag_components() -> CondensePlusContextChatEngine:
    """RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã€ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’è¿”ã—ã¾ã™ã€‚"""

    api_key = settings.GEMINI_API_KEY
    print(f"DEBUG: API Key Loaded (starts with: {api_key[:5]})")
    
    # 2.1. Gemini Clientã®åˆæœŸåŒ–ã¨èªè¨¼ãƒã‚§ãƒƒã‚¯
    try:
        # LLM Client/Embedding Model ã« APIã‚­ãƒ¼ã‚’ç›´æ¥æ¸¡ã™
        llm_client = Gemini(model="gemini-2.5-flash", api_key=api_key)
        embed_model = GeminiEmbedding(model_name="text-embedding-004", api_key=api_key)
        
        print("INFO: Gemini LLM/Embedding Client Initialization Successful.")
        
    except Exception as e:
        print(f"FATAL ERROR: Gemini Clientã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {e}")
        sys.exit(1) 

    # 2.2. LlamaIndexã®ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
    # ğŸš¨ ServiceContextã®ä»£ã‚ã‚Šã«Settingsã«ç›´æ¥è¨­å®šã—ã¾ã™ ğŸš¨

    Settings.llm = llm_client
    Settings.embed_model = embed_model
    

    # 2.3. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰
    try:
        documents = SimpleDirectoryReader("./docs").load_data()
    except Exception as e:
        print(f"WARNING: 'docs'ãƒ•ã‚©ãƒ«ãƒ€ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚{e}")
        documents = []

    if not documents:
        print("WARNING: RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ç´”ç²‹ãªGeminiãƒãƒ£ãƒƒãƒˆã¨ã—ã¦å‹•ä½œã—ã¾ã™ã€‚")
        # SettingsãŒæœ‰åŠ¹ãªãŸã‚ã€å¼•æ•°ãªã—ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        index = VectorStoreIndex([], embed_model=embed_model)
    else:
        index = VectorStoreIndex.from_documents(
            documents, 
            embed_model=embed_model # ğŸ‘ˆ embed_model ã‚’ç›´æ¥æ¸¡ã™
        )
        print(f"INFO: RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")

    # 2.4. RAGãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã®ä½œæˆ
    chat_engine = CondensePlusContextChatEngine.from_defaults(
        retriever=index.as_retriever(),
        llm=llm_client, 
        system_prompt="ã‚ãªãŸã¯æä¾›ã•ã‚ŒãŸçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ã„ã¦ã®ã¿å›ç­”ã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’ä¸å¯§ã«ä¼ãˆã¦ãã ã•ã„ã€‚",
    )

    return chat_engine




# ğŸš¨ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«RAGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä¸€åº¦ã ã‘åˆæœŸåŒ– ğŸš¨
try:
    RAG_CHAT_ENGINE = initialize_rag_components()
except Exception as e:
    raise RuntimeError(f"RAGã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# --- 3. FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®šç¾© ---
app = FastAPI()
chat_engines: Dict[str, BaseChatEngine] = {}

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    user_id: str
    message: str


# --- 4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ (éåŒæœŸã‚¨ãƒ©ãƒ¼å›é¿ç‰ˆ) ---
async def generate_rag_stream(engine: BaseChatEngine, prompt: str):
    """LlamaIndexã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‡¦ç†ã™ã‚‹éåŒæœŸã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿"""
    try:
        # éåŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã€å¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        response_stream = await engine.astream_chat(prompt) 

        # å¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå†…ã®é€šå¸¸ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ (.response_gen) ã‚’é€šå¸¸ã® for ã§å‡¦ç†
        # éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¶­æŒã™ã‚‹ãŸã‚ã€ãƒ«ãƒ¼ãƒ—å†…ã§ await asyncio.sleep(0) ã‚’å®Ÿè¡Œ
        for token in response_stream.response_gen:
            if token:
                yield token 
                await asyncio.sleep(0)

    except Exception as e:
        print(f"RAG/APIã‚¨ãƒ©ãƒ¼: {e}")
        yield f"\n[ERROR] RAG/APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"


# --- 5. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    user_id = request.user_id
    prompt = request.message
    
    if user_id not in chat_engines:
        # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã€å±¥æ­´ã‚’ç‹¬ç«‹ã•ã›ã‚‹
        chat_engines[user_id] = RAG_CHAT_ENGINE
        print(f"æ–°è¦RAGã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹: {user_id}")
    
    chat_engine = chat_engines[user_id]
    
    return StreamingResponse(
        generate_rag_stream(chat_engine, prompt),
        media_type="text/plain" 
    )